{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # MTH8408 : Méthodes d'optimisation et contrôle optimal\n",
    " ## Laboratoire 3: Optimisation sans contraintes et méthodes itératives\n",
    "Tangi Migot et Paul Raynaud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra, NLPModels, Printf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADNLPModel - Model with automatic differentiation backend ADModelBackend{\n",
       "  ForwardDiffADGradient,\n",
       "  ForwardDiffADHvprod,\n",
       "  EmptyADbackend,\n",
       "  EmptyADbackend,\n",
       "  EmptyADbackend,\n",
       "  ForwardDiffADHessian,\n",
       "  EmptyADbackend,\n",
       "}\n",
       "  Problem name: Generic\n",
       "   All variables: ████████████████████ 2      All constraints: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "            free: ████████████████████ 2                 free: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "           lower: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                lower: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "           upper: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                upper: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "         low/upp: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0              low/upp: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "           fixed: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                fixed: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "          infeas: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0               infeas: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "            nnzh: (  0.00% sparsity)   3               linear: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "                                                    nonlinear: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "                                                         nnzj: (------% sparsity)         \n",
       "\n",
       "  Counters:\n",
       "             obj: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                 grad: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                 cons: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "        cons_lin: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0             cons_nln: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                 jcon: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "           jgrad: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                  jac: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0              jac_lin: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "         jac_nln: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                jprod: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0            jprod_lin: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "       jprod_nln: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0               jtprod: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0           jtprod_lin: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "      jtprod_nln: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                 hess: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                hprod: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "           jhess: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0               jhprod: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Test problem:\n",
    "using ADNLPModels\n",
    "fH(x) = (x[2]+x[1].^2-11).^2+(x[1]+x[2].^2-7).^2\n",
    "x0H = [10., 20.]\n",
    "himmelblau = ADNLPModel(fH, x0H)\n",
    "\n",
    "problem2 = ADNLPModel(x->-x[1]^2, ones(3))\n",
    "\n",
    "roz(x) = 100 *  (x[2] - x[1]^2)^2 + (x[1] - 1.0)^2\n",
    "rosenbrock = ADNLPModel(roz, [-1.2, 1.0])\n",
    "\n",
    "f(x) = x[1]^2 * (2*x[1] - 3) - 6*x[1]*x[2] * (x[1] - x[2] - 1)\n",
    "pb_du_cours = ADNLPModel(f, [-1.001, -1.001]) #ou [1.5, .5] ou [.5, .5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commentaires sur Julia\n",
    "\n",
    "Quelques commentaires sur des morceaux de codes que vous avez vu:\n",
    "- les structures, exemple [GenericExecutionStats](https://github.com/JuliaSmoothOptimizers/SolverCore.jl/blob/0091f437a26a27ac8aa53d5e37647223722f7f7c/src/stats.jl#L60) (constructeur, attribut, type).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "using SolverCore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "syntax: invalid identifier name \"?\"",
     "output_type": "error",
     "traceback": [
      "syntax: invalid identifier name \"?\"\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Desktop/MTH8408/Git/MTH8408-Hiv24/lab3/Lab3-notebook.ipynb:1"
     ]
    }
   ],
   "source": [
    "? GenericExecutionStats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Les arguments dans les fonctions. Lire attentivement [la documentation Julia sur les fonctions](https://docs.julialang.org/en/v1/manual/functions/) pour comprendre l'utilisation des `Optional Arguments` et des `Keywords Arguments`. Ce type d'arguments est très utile dans nos applictions où les solveurs dépendent de paramètre dont on peut fixer des valeurs par défaut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1: Méthode BFGS avec mémoire limitée (L-BFGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but de cet exercice est d'implémenter la méthode BFGS à mémoire limitée vue en cours en utilisant les `InverseLBFGSOperator` du package `LinearOperators.jl`. Il y a aussi un petit exemple dans la documentation du package [LinearOperators.jl/dev/tutorial/#Limited-memory-BFGS-and-SR1](https://juliasmoothoptimizers.github.io/LinearOperators.jl/dev/tutorial/#Limited-memory-BFGS-and-SR1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearOperators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? InverseLBFGSOperator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ce qui est important dans ce type de méthode est:\n",
    "- le paramètre mémoire\n",
    "- la mise à jour de l'opérateur avec la fonction `push!`\n",
    "- si on a pas une direction de descente, alors on skip\n",
    "- recherche linéaire d'Armijo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? LinearOperators.push!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "armijo (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function armijo(xk, dk, fk, gk, slope, nlp :: AbstractNLPModel; τ1 = 1.0e-4, t_update = 1.5)\n",
    "  t = 1.0\n",
    "  fk_new = obj(nlp, xk + dk) # t = 1.0\n",
    "  while fk_new > fk + τ1 * t * slope\n",
    "    t /= t_update\n",
    "    fk_new = obj(nlp, xk + t * dk)\n",
    "  end\n",
    "  return t, fk_new\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "limited_bfgs (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function limited_bfgs(nlp      :: AbstractNLPModel;\n",
    "                      x        :: AbstractVector = nlp.meta.x0,\n",
    "                      atol     :: Real = √eps(eltype(x)), \n",
    "                      rtol     :: Real = √eps(eltype(x)),\n",
    "                      max_eval :: Int = -1,\n",
    "                      max_time :: Float64 = 30.0,\n",
    "                      f_min    :: Float64 = -1.0e16,\n",
    "                      verbose  :: Bool = true,\n",
    "                      mem      :: Int = 5)\n",
    "  start_time = time()\n",
    "  elapsed_time = 0.0\n",
    "\n",
    "  T = eltype(x)\n",
    "  n = nlp.meta.nvar\n",
    "\n",
    "  xt = zeros(T, n)\n",
    "  ∇ft = zeros(T, n)\n",
    "\n",
    "  f = obj(nlp, x)\n",
    "  ∇f = grad(nlp, x)\n",
    "#################################################\n",
    "  H = InverseLBFGSOperator(T,n)\n",
    "#################################################\n",
    "\n",
    "  ∇fNorm = norm(∇f) #nrm2(n, ∇f)\n",
    "  ϵ = atol + rtol * ∇fNorm\n",
    "  iter = 0\n",
    "\n",
    "  @info log_header([:iter, :f, :dual, :slope, :bk], [Int, T, T, T, T],\n",
    "                   hdr_override=Dict(:f=>\"f(x)\", :dual=>\"‖∇f‖\", :slope=>\"∇fᵀd\"))\n",
    "\n",
    "  optimal = ∇fNorm ≤ ϵ\n",
    "  unbdd = f ≤ f_min\n",
    "  tired = neval_obj(nlp) > max_eval ≥ 0 || elapsed_time > max_time\n",
    "  stalled = false\n",
    "  status = :unknown\n",
    "\n",
    "  while !(optimal || tired || stalled || unbdd)\n",
    "\n",
    "#################################################\n",
    "    d = -H*∇f\n",
    "#################################################\n",
    "    slope = dot(d, ∇f)\n",
    "    if slope ≥ 0\n",
    "      @error \"not a descent direction\" slope\n",
    "      status = :not_desc\n",
    "      stalled = true\n",
    "      continue\n",
    "    end\n",
    "\n",
    "    # Perform improved Armijo linesearch.\n",
    "    t, ft = armijo(x, d, f, ∇f, slope, nlp)\n",
    "        \n",
    "    @info log_row(Any[iter, f, ∇fNorm, slope, t])\n",
    "\n",
    "    # Update L-BFGS approximation.\n",
    "    xt = x + t * d\n",
    "    ∇ft = grad(nlp, xt) # grad!(nlp, xt, ∇ft)\n",
    "#################################################\n",
    "    push!(H,xt-x,∇ft-∇f)\n",
    "#################################################\n",
    "\n",
    "    # Move on.\n",
    "    x = xt\n",
    "    f = ft\n",
    "    ∇f = ∇ft\n",
    "\n",
    "    ∇fNorm = norm(∇f) #nrm2(n, ∇f)\n",
    "    iter = iter + 1\n",
    "\n",
    "    optimal = ∇fNorm ≤ ϵ\n",
    "    unbdd = f ≤ f_min\n",
    "    elapsed_time = time() - start_time\n",
    "    tired = neval_obj(nlp) > max_eval ≥ 0 || elapsed_time > max_time\n",
    "  end\n",
    "  @info log_row(Any[iter, f, ∇fNorm])\n",
    "\n",
    "  if optimal\n",
    "    status = :first_order\n",
    "  elseif tired\n",
    "    if neval_obj(nlp) > max_eval ≥ 0\n",
    "      status = :max_eval\n",
    "    elseif elapsed_time > max_time\n",
    "      status = :max_time\n",
    "    end\n",
    "  elseif unbdd\n",
    "        status = :unbounded\n",
    "  end\n",
    "\n",
    "  return GenericExecutionStats(\n",
    "        nlp,\n",
    "        status=status,\n",
    "        solution=x,\n",
    "        objective=f,\n",
    "        dual_feas=∇fNorm,\n",
    "        iter=iter,\n",
    "        elapsed_time=elapsed_time,\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unit/Validation Tests\n",
    "# Réaliser un test unitaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(stats.status, stats.solution) = (:first_order, [3.584428266659278, -1.8481265666485829])\n",
      "(stats.status, stats.solution) = "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(:unbounded, [1.29140163e8, 1.0, 1.0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(stats.status, stats.solution) = (:first_order, [0.9999999887950609, 0.9999999782159007])\n",
      "(stats.status, stats.solution) = "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(:unbounded, [-975544.6831042847, -764227.9248199855])\n",
      "(stats.status, stats.solution) = (:first_order, [0.9999999962625671, -3.1168150200102845e-9])\n",
      "(stats.status, stats.solution) = (:first_order, [0.99999999073849, -6.617373493448244e-9])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[1mTest Summary: | \u001b[22m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal  \u001b[22m\u001b[39m\u001b[0m\u001b[1mTime\u001b[22m\n",
      "test set      | \u001b[32m   9  \u001b[39m\u001b[36m    9  \u001b[39m\u001b[0m3.4s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Test.DefaultTestSet(\"test set\", Any[], 9, false, false, true, 1.707162119081548e9, 1.70716212243425e9, false)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "include(\"test-lbfgs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus:\n",
    "\n",
    "- Compare l'implémentation de `limited_bfgs` avec la fonction `lbfgs` qui est disponible dans `JSOSolvers.jl`.\n",
    "- On veut pouvoir tester \"facilement\" plusieurs valeurs de $\\tau$ et du paramètre de mise à jour dans `armijo` sur les problèmes tests. Comment modifier le code pour que ça soit possible?\n",
    "\n",
    "On peut mesurer deux executions de fonctions Julia grâce aux fonctions de `BenchmarkTools.jl`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools\n",
    "using JSOSolvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? @time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lbfgs(himmelblau) :: 0.000113 seconds (269 allocations: 10.773 KiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "limited_bfgs(himmelblau) :: 0.015503 seconds (1.82 k allocations: 171.609 KiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info:   iter      f(x)      ‖∇f‖      ∇fᵀd        bk  \n",
      "└ @ Main /Users/jules/Desktop/MTH8408/Git/MTH8408-Hiv24/lab3/Lab3-notebook.ipynb:29\n",
      "┌ Info:      0   1.7e+05   3.3e+04  -1.1e+09   1.0e-03\n",
      "└ @ Main /Users/jules/Desktop/MTH8408/Git/MTH8408-Hiv24/lab3/Lab3-notebook.ipynb:54\n",
      "┌ Info:      1   2.7e+04   8.6e+03  -5.9e+04   1.0e+00\n",
      "└ @ Main /Users/jules/Desktop/MTH8408/Git/MTH8408-Hiv24/lab3/Lab3-notebook.ipynb:54\n",
      "┌ Info:      2   1.3e+03   8.6e+02  -6.6e+02   1.0e+00\n",
      "└ @ Main /Users/jules/Desktop/MTH8408/Git/MTH8408-Hiv24/lab3/Lab3-notebook.ipynb:54\n",
      "┌ Info:      3   7.2e+02   5.7e+02  -8.8e+02   1.0e+00\n",
      "└ @ Main /Users/jules/Desktop/MTH8408/Git/MTH8408-Hiv24/lab3/Lab3-notebook.ipynb:54\n",
      "┌ Info:      4   1.6e+02   2.0e+02  -1.6e+02   1.0e+00\n",
      "└ @ Main /Users/jules/Desktop/MTH8408/Git/MTH8408-Hiv24/lab3/Lab3-notebook.ipynb:54\n",
      "┌ Info:      5   4.6e+01   8.6e+01  -5.4e+01   1.0e+00\n",
      "└ @ Main /Users/jules/Desktop/MTH8408/Git/MTH8408-Hiv24/lab3/Lab3-notebook.ipynb:54\n",
      "┌ Info:      6   1.0e+01   3.3e+01  -1.3e+01   1.0e+00\n",
      "└ @ Main /Users/jules/Desktop/MTH8408/Git/MTH8408-Hiv24/lab3/Lab3-notebook.ipynb:54\n",
      "┌ Info:      7   1.8e+00   1.2e+01  -2.6e+00   1.0e+00\n",
      "└ @ Main /Users/jules/Desktop/MTH8408/Git/MTH8408-Hiv24/lab3/Lab3-notebook.ipynb:54\n",
      "┌ Info:      8   1.9e-01   3.7e+00  -3.8e-01   1.0e+00\n",
      "└ @ Main /Users/jules/Desktop/MTH8408/Git/MTH8408-Hiv24/lab3/Lab3-notebook.ipynb:54\n",
      "┌ Info:      9   4.7e-02   3.1e+00  -1.1e-01   1.0e+00\n",
      "└ @ Main /Users/jules/Desktop/MTH8408/Git/MTH8408-Hiv24/lab3/Lab3-notebook.ipynb:54\n",
      "┌ Info:     10   2.0e-03   5.7e-01  -3.8e-03   1.0e+00\n",
      "└ @ Main /Users/jules/Desktop/MTH8408/Git/MTH8408-Hiv24/lab3/Lab3-notebook.ipynb:54\n",
      "┌ Info:     11   1.4e-06   9.4e-03  -2.7e-06   1.0e+00\n",
      "└ @ Main /Users/jules/Desktop/MTH8408/Git/MTH8408-Hiv24/lab3/Lab3-notebook.ipynb:54\n",
      "┌ Info:     12   4.6e-09   7.0e-04  -9.2e-09   1.0e+00\n",
      "└ @ Main /Users/jules/Desktop/MTH8408/Git/MTH8408-Hiv24/lab3/Lab3-notebook.ipynb:54\n",
      "┌ Info:     13   3.3e-13   8.2e-06\n",
      "└ @ Main /Users/jules/Desktop/MTH8408/Git/MTH8408-Hiv24/lab3/Lab3-notebook.ipynb:76\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Execution stats: first-order stationary\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@time \"lbfgs(himmelblau) :\" lbfgs(himmelblau)\n",
    "@time \"limited_bfgs(himmelblau) :\" limited_bfgs(himmelblau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2: NewtonCG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but de cet exercice est d'adapter les méthodes de Newton de façon à résoudre le système linéaire avec une méthode itérative de type gradient conjugué comme suit ($B_k$ représente la matrice hessienne):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"LineSearchNewtonCG.png\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cg_optim (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function cg_optim(H, ∇f)\n",
    "    #setup the tolerance:\n",
    "    n∇f = norm(∇f)\n",
    "#####################################\n",
    "    ϵk = min(0.5, √n∇f)*n∇f\n",
    "####################################\n",
    "    n = length(∇f)\n",
    "    z = zeros(n)\n",
    "    r = ∇f\n",
    "    d = -r\n",
    "    \n",
    "    j = 0\n",
    "    while norm(r) ≥ ϵk && j < 3 * n\n",
    "###############################################\n",
    "        if dot(d, H * d) ≤ 0\n",
    "            if j == 0\n",
    "                p = d\n",
    "            else\n",
    "                p = z \n",
    "            end\n",
    "        end\n",
    "##############################################\n",
    "        α = dot(r,r)/dot(d,H*d)\n",
    "##############################################        \n",
    "        z += α * d\n",
    "        nrr2 = dot(r, r)\n",
    "        r += α * H * d\n",
    "##############################################\n",
    "        β  = dot(r, r)/nrr2\n",
    "##############################################\n",
    "        d  = -r + β * d\n",
    "        j += 1\n",
    "    end\n",
    "    return z\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce qui est important ici est qu'on a pas besoin de stocker/évaluer la matrice hessienne entière mais simplement le produit entre la hessienne et un vecteur. Pour un `NLPModels` on utilise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "using NLPModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? NLPModels.hprod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? NLPModels.hess_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function armijo_Newton_cg(nlp      :: AbstractNLPModel;\n",
    "                          x        :: AbstractVector = nlp.meta.x0,\n",
    "                          atol     :: Real = √eps(eltype(x)), \n",
    "                          rtol     :: Real = √eps(eltype(x)),\n",
    "                          max_eval :: Int = -1,\n",
    "                          max_time :: Float64 = 30.0,\n",
    "                          f_min    :: Float64 = -1.0e16)\n",
    "  start_time = time()\n",
    "  elapsed_time = 0.0\n",
    "\n",
    "  T = eltype(x)\n",
    "  n = nlp.meta.nvar\n",
    "\n",
    "  f = obj(nlp, x)\n",
    "  ∇f = grad(nlp, x)\n",
    "#################################################\n",
    "  H = # Initialize H as linear operator representing the Hessian matrix\n",
    "#################################################\n",
    "\n",
    "  ∇fNorm = norm(∇f) #nrm2(n, ∇f)\n",
    "  ϵ = atol + rtol * ∇fNorm\n",
    "  iter = 0\n",
    "\n",
    "  @info log_header([:iter, :f, :dual, :slope, :bk], [Int, T, T, T, T],\n",
    "                   hdr_override=Dict(:f=>\"f(x)\", :dual=>\"‖∇f‖\", :slope=>\"∇fᵀd\"))\n",
    "\n",
    "  optimal = ∇fNorm ≤ ϵ\n",
    "  unbdd = f ≤ f_min\n",
    "  tired = neval_obj(nlp) > max_eval ≥ 0 || elapsed_time > max_time\n",
    "  stalled = false\n",
    "  status = :unknown\n",
    "\n",
    "  while !(optimal || tired || stalled || unbdd)\n",
    "        \n",
    "    d = cg_optim(H, ∇f)\n",
    "        \n",
    "    slope = dot(d, ∇f)\n",
    "    if slope ≥ 0\n",
    "      @error \"not a descent direction\" slope\n",
    "      status = :not_desc\n",
    "      stalled = true\n",
    "      continue\n",
    "    end\n",
    "\n",
    "    # Perform improved Armijo linesearch.\n",
    "    t, f = armijo(x, d, f, ∇f, slope, nlp)\n",
    "        \n",
    "    @info log_row(Any[iter, f, ∇fNorm, slope, t])\n",
    "\n",
    "    # Update L-BFGS approximation.\n",
    "    x += t * d\n",
    "    ∇f = grad(nlp, x)\n",
    "#################################################\n",
    "    H = ### Update H\n",
    "#################################################\n",
    "\n",
    "    ∇fNorm = norm(∇f) #nrm2(n, ∇f)\n",
    "    iter = iter + 1\n",
    "\n",
    "    optimal = ∇fNorm ≤ ϵ\n",
    "    unbdd = f ≤ f_min\n",
    "    elapsed_time = time() - start_time\n",
    "    tired = neval_obj(nlp) > max_eval ≥ 0 || elapsed_time > max_time\n",
    "  end\n",
    "  @info log_row(Any[iter, f, ∇fNorm])\n",
    "\n",
    "  if optimal\n",
    "    status = :first_order\n",
    "  elseif tired\n",
    "    if neval_obj(nlp) > max_eval ≥ 0\n",
    "      status = :max_eval\n",
    "    elseif elapsed_time > max_time\n",
    "      status = :max_time\n",
    "    end\n",
    "  elseif unbdd\n",
    "        status = :unbounded\n",
    "  end\n",
    "\n",
    "  return GenericExecutionStats(nlp, status = status, solution=x, objective=f, dual_feas=∇fNorm,\n",
    "                               iter=iter, elapsed_time=elapsed_time)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unit/Validation Tests\n",
    "# Réaliser un test unitaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment préparer un benchmark\n",
    "\n",
    "On veut maintenant pouvoir réaliser un benchmark de plusieurs solveurs. Pour comparer les algorithmes, il nous faut une collection de problèmes tests et on va utiliser `OptimizationProblems.jl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using OptimizationProblems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez trouver un tutoriel de comment réaliser un benchmark avec ce package sur la documentation [OptimizationProblems.jl/dev/benchmark/](https://juliasmoothoptimizers.github.io/OptimizationProblems.jl/dev/benchmark/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est fort possible que les petits problèmes tests que l'on résout après l'implémentation ne suffisent pas à déceler des bugs. Mais on peut toujours analyser l'éxecution de notre algorithme sur certains problèmes de la collection afin d'améliorer la valeur de certains paramètres (limite de temps, d'itérations, d'évaluations), détecter un bug, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using OptimizationProblems.PureJuMP, NLPModelsJuMP\n",
    "jump_model = AMPGO02() # OptimizationProblems.PureJuMP.AMPGO02\n",
    "prbl = MathOptNLPModel(jump_model)\n",
    "limited_bfgs(prbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous vous en doutez pour le rapport de cette semaine on va vouloir réaliser une benchmark avec les deux méthodes que l'on a codé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix:\n",
    "\n",
    "Une petite remarque sur la gestion de la mémoire:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = 1\n",
      "b = 1\n",
      "b = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Pour les nombres:\n",
    "a = 1\n",
    "@show a\n",
    "b = a\n",
    "@show b\n",
    "a = 2\n",
    "@show b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pour les tableaux:\n",
    "a = zeros(Float64, 2) #or zeros(2)\n",
    "@show a\n",
    "b = a\n",
    "@show (a,b)\n",
    "a = ones(Float64, 2)\n",
    "@show (a,b)\n",
    "\n",
    "#Pour les tableaux:\n",
    "a = ones(Float64, 2)\n",
    "b = a\n",
    "@show (a,b)\n",
    "a .= 2*ones(Float64, 2) #same would go with grad!\n",
    "@show (a,b)\n",
    "\n",
    "#Pour les tableaux:\n",
    "a = ones(Float64, 2)\n",
    "b = copy(a) # or similar(a)\n",
    "@show (a,b)\n",
    "a .= 2 .* ones(Float64, 2) #same would go with grad!\n",
    "@show (a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pour les NLPModels, il existe aussi des fonctions qui interviennent sur la mémoire\n",
    "gk = grad(nlp, x0)\n",
    "grad!(nlp, x0, gk) #équivaut à gk .= grad(nlp, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparez les `grad` et `grad!` à l'aide de `@benchmark` ainsi que:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000\n",
    "a = rand(n)\n",
    "b = ones(n)\n",
    "c = similar(a)\n",
    "@benchmark c = 2 * a + b * 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark c .= 2 .* a .+ b .* 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En particulier, observez la mémoire allouée et le temps moyen requis pour performer la ligne d'instruction.\n",
    "La mémoire nécessaire varie en fonction du nombre et du type d'opération effectué.\n",
    "Parmis les opérations allouant le moins de mémoire, on retrouve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark c .= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour manipuler plus finement la mémoire au cours de votre implémentation, vous pouvez utilisez des fonctions de `LinearAlgebra`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? axpy!\n",
    "? axpby!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qui sont des routines `BLAS`.\n",
    "Faites attention, ces routines ont des effets de bord sur les structures de données `y`.\n",
    "Dès lors, l'utilisation de `@benchmark` accumule successivement les effets de bord, d'où l'utilisation des tests avant `@benchmark`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = zeros(n)\n",
    "x = ones(n)\n",
    "a = 2\n",
    "axpy!(a, x, y)\n",
    "@show y == a * x\n",
    "\n",
    "@benchmark axpy!(a, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = π * ones(n)\n",
    "x = ones(n)\n",
    "a = 2\n",
    "b = 3\n",
    "axpby!(a, x, b, y)\n",
    "@show y == a * ones(n) + b * π * ones(n)\n",
    "\n",
    "@benchmark axpby!(a, x, b, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cela fontionne aussi pour les matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = π * ones(n,n)\n",
    "x = ones(n,n)\n",
    "a = 2\n",
    "b = 3\n",
    "axpby!(a, x, b, y)\n",
    "@show y == a * ones(n,n) + b * π * ones(n,n)\n",
    "\n",
    "@benchmark axpby!(a, x, b, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
